{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRISP_ENDPOINT = 'http://crisp.ai.wu.ac.at/blazegraph/namespace/crisp/sparql'\n",
    "OLLAMA_ENDPOINT = 'http://llama-max-ollama.ai.wu.ac.at'\n",
    "CRISP_NAMESPACE = 'http://crisp.ai.wu.ac.at/crisp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparql_query(query: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executes a SPARQL query on a pre-loaded RDF graph and returns the results as a DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare and execute the query        \n",
    "        # query = prepareQuery(query)\n",
    "        # results = rdf_graph.query(query)\n",
    "\n",
    "        response = requests.get(CRISP_ENDPOINT, params={'query': query, 'format': 'json'})\n",
    "        results = response.json()\n",
    "\n",
    "        \n",
    "        # Extract variable (column) names from the query result\n",
    "        columns = results['head']['vars']  # Get the variable names from the query results\n",
    "        \n",
    "        # Process the results and convert them into a list of dictionaries\n",
    "        data = []\n",
    "        for row in results['results']['bindings']:\n",
    "            row_data = {str(var): row[var]['value'].replace(CRISP_NAMESPACE, \"\") for var in columns}  # Dynamically build a row dict\n",
    "            # .replace(CRISP_NS, \"\")\n",
    "            data.append(row_data)\n",
    "        \n",
    "        # Convert the data into a DataFrame\n",
    "        df = pd.DataFrame(data, columns=[str(var) for var in columns])\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while executing the SPARQL query: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode(\"Why sky is blue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community information\n",
    "community_query = \"\"\"\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "prefix crisp: <http://crisp.ai.wu.ac.at/crisp/>\n",
    "\n",
    "SELECT ?community_id ?community_name ?state_name\n",
    "\n",
    "WHERE {\n",
    "  ?community_id a crisp:Community;\n",
    "    rdfs:label ?community_name ; \n",
    "    crisp:locatedIn ?district_id .\n",
    "  ?district_id crisp:locatedIn ?state_id .\n",
    "  ?state_id rdfs:label ?state_name .      \n",
    "} LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Observations of a community\n",
    "def observation_query(community_id):\n",
    "  return f\"\"\"\n",
    "    prefix sosa: <http://www.w3.org/ns/sosa/> \n",
    "    prefix crisp: <http://crisp.ai.wu.ac.at/crisp/>\n",
    "\n",
    "    SELECT ?observation_id ?property ?value\n",
    "\n",
    "    WHERE {{\n",
    "      ?observation_id sosa:hasFeatureOfInterest <http://crisp.ai.wu.ac.at/crisp/{community_id}> ;\n",
    "          sosa:observedProperty ?property; \n",
    "          sosa:hasSimpleResult ?value . \n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma Persistent Client\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_or_create_collection(name=\"graphrag_collection\")\n",
    "\n",
    "# Prepare data for Chroma collection\n",
    "documents, metadatas, embeddings, ids = [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adding communities to Vector DB\n",
    "community_df = sparql_query(community_query)\n",
    "print(community_df)\n",
    "\n",
    "\n",
    "# Using tqdm for progress tracking\n",
    "print(\"Processing communities and adding to Chroma collection...\")\n",
    "\n",
    "communities = {}\n",
    "for _, row in tqdm(community_df.iterrows(), total=len(community_df), desc=\"Communities Processed\"):\n",
    "    description = f\"{row['community_name']}  is a community located in {row['state_name']} state in Austria\"\n",
    "    description_embedding = model.encode(description)\n",
    "    documents.append(f\"Name: {row['community_name']}, Type: 'Community', Description: {description}\")\n",
    "    metadatas.append({\n",
    "        'subject': row['community_id'],\n",
    "        'name': str(row['community_name']),\n",
    "        'type': 'Community',\n",
    "        'description': description\n",
    "    })\n",
    "    # embeddings.append([float(x) for x in row['description_embedding'].split()])\n",
    "    embeddings.append(description_embedding)\n",
    "    ids.append(str(row['community_id']))\n",
    "    communities[row['community_id']] = row['community_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_description(community_name, row):\n",
    "    if row['property'] == \"weeklyHeatdaysOver30\":\n",
    "        year, week = row['observation_id'].split('/')[-2:]\n",
    "        return f\"The {community_name} community experienced {row['value']} hot days during week {week} of {year}\"\n",
    "    else:\n",
    "        year = row['observation_id'].split('/')[-1]\n",
    "        return f\"The population of the {community_name} community in {year} was {row['value']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding community observations to Vector DB\n",
    "for community_id in communities:\n",
    "  observation_df = sparql_query(observation_query(community_id))\n",
    "\n",
    "  # Using tqdm for progress tracking\n",
    "  print(\"Processing observations and adding to Chroma collection...\")\n",
    "\n",
    "  for _, row in tqdm(observation_df.iterrows(), total=len(observation_df), desc=\"Observations Processed\"):\n",
    "\n",
    "      \n",
    "      description = observation_description(communities[community_id], row)\n",
    "      description_embedding = model.encode(description)\n",
    "      documents.append(f\"Type: 'Observation', Description: {description}\")\n",
    "      metadatas.append({\n",
    "          'subject': row['observation_id'],\n",
    "          'type': 'Observation',\n",
    "          'description': description\n",
    "      })\n",
    "      embeddings.append(description_embedding)\n",
    "      ids.append(str(row['observation_id']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the processed data to the Chroma collection\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    embeddings=embeddings,\n",
    "    ids=ids\n",
    ")\n",
    "print(\"Chroma collection populated successfully.\")\n",
    "\n",
    "# Initialize vector store using Chroma\n",
    "vector_store = Chroma(client=persistent_client, collection_name=\"graphrag_collection\")\n",
    "\n",
    "# Verify the count of entries in the collection\n",
    "print(f\"Total entries in the collection: {vector_store._collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model = \"llama3.1\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    "    base_url = OLLAMA_ENDPOINT\n",
    "    # other params ...\n",
    ")\n",
    "\n",
    "community_id, community_name = list(communities.items())[0]\n",
    "\n",
    "question = f\"What can you tell me about hot days in {community_name}?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query without context\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that answers questions about community observations.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input\": question,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate embedding of query\n",
    "query_embedding_vector = model.encode(question)\n",
    "print(query_embedding_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_ENTITIES = 10\n",
    "TOP_CHUNKS = 10\n",
    "TOP_COMMUNITIES = 3\n",
    "TOP_OUTGOING_RELATIONSHIPS = 10\n",
    "TOP_INCOMING_RELATIONSHIPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search_by_vector(\n",
    "    embedding=query_embedding_vector, k=TOP_ENTITIES\n",
    ")\n",
    "entity_list = [doc.metadata['subject'] for doc in results]\n",
    "descriptions = [doc.metadata['description'] for doc in results]\n",
    "\n",
    "context = \". \\n\".join(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with context from embeddings\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "         (\n",
    "            \"system\",\n",
    "            f\"You are a helpful assistant that answers questions about community observations. You have the following context: {context}\",\n",
    "        ),\n",
    "       (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input\": question,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
